{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================  1\n",
      "generating lake...\n",
      "finished reading data\n",
      "lake shape:  (800, 256)\n",
      "virus shape:  (5776, 256)\n",
      "bacteria shape:  (5242, 256)\n",
      "running knn with  110  neighbors....\n",
      "indices:  800 110\n",
      "right answers:  726  /  800\n",
      "=====================  6\n",
      "generating lake...\n",
      "finished reading data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df6ef131319f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlake\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mlake_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-df6ef131319f>\u001b[0m in \u001b[0;36mFeatureVector\u001b[0;34m(dictionary, sequence, n)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adding this snippet so the code can run on osx\n",
    "from sys import platform as platform_name\n",
    "if platform_name == \"darwin\":\n",
    "   import sys\n",
    "   sys.path.append('//anaconda/lib/python3.5/site-packages/')\n",
    "\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from string import ascii_uppercase\n",
    "\n",
    "from swalign import swalign\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "from numpy import trapz\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import threading\n",
    "\n",
    "import generate_fake_lake as fl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ****** aux classes ********\n",
    "class featureVectorThread (threading.Thread):\n",
    "    def __init__(self, matrix, i, D, w, n):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = i\n",
    "        self.dict = D\n",
    "        self.sequence = w\n",
    "        self.n = n\n",
    "        self.matrix = matrix\n",
    "    def run(self):\n",
    "        print(\"Starting \", self.i)\n",
    "        arr = FeatureVector(D, str(w), n)\n",
    "        arr = np.divide(np.array(arr), len(w))\n",
    "        matrix[i] = arr\n",
    "        print(\"Exiting \", self.i)\n",
    "\n",
    "# ****** functions ********\n",
    "\n",
    "#read n .fna database files in the specified path\n",
    "#set n = 0 to read all files\n",
    "def ReadDataBase(_path, n):\n",
    "    seqList = []\n",
    "    from os import path\n",
    "    files = os.listdir(_path) #makes a list of all files in folder\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for f in files:\n",
    "        for seq_record in SeqIO.parse(_path + f, \"fasta\"):\n",
    "            seqList.append(seq_record.seq) # reads each file into a list\n",
    "            j += 1\n",
    "            if(n > 0):\n",
    "                if(j > n-1):\n",
    "                    i = n\n",
    "                    break\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "        if(n > 0):\n",
    "            if(i > n-1):\n",
    "                break\n",
    "    return seqList\n",
    "\n",
    "\n",
    "#read n .fna database files in the specified path\n",
    "#set n = 0 to read all files\n",
    "def ReadDataBaseFilenames(_path, n, filename_filename):\n",
    "    seqList = []\n",
    "    filenameList = []\n",
    "    from os import path\n",
    "    files = os.listdir(_path) #makes a list of all files in folder\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for f in files:\n",
    "        for seq_record in SeqIO.parse(_path + f, \"fasta\"):\n",
    "            s = seq_record.seq\n",
    "            seqList.append(s) # reads each file into a list\n",
    "            filenameList.append(_path + f)\n",
    "            j += 1\n",
    "            if(n > 0):\n",
    "                if(j > n-1):\n",
    "                    i = n\n",
    "                    break\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "        if(n > 0):\n",
    "            if(i > n-1):\n",
    "                break\n",
    "\n",
    "    fid = open(filename_filename, 'w')\n",
    "    for f in filenameList:\n",
    "        fid.write(f + '\\n')\n",
    "    fid.close()\n",
    "    return seqList, filenameList\n",
    "\n",
    "\n",
    "#creates dictionary with all permutations of length n\n",
    "#with repetition to index Feature Vector\n",
    "def CreateDictionary(n):\n",
    "    chars = \"ACGT\"\n",
    "    arr = list(itertools.product(chars, repeat=n))\n",
    "\n",
    "    D = {}\n",
    "    i = 0\n",
    "\n",
    "    for a in arr:\n",
    "        D[''.join(a)] = i\n",
    "        i += 1\n",
    "\n",
    "    return D\n",
    "\n",
    "#builds the feature vector for sequence using specified indexing dictionary\n",
    "def FeatureVector(dictionary, sequence, n):\n",
    "    sLen = len(sequence)\n",
    "    arr = [0]*4**n\n",
    "    i = 0\n",
    "    while(1):\n",
    "        w = sequence[i:i+n]\n",
    "        try:\n",
    "            arr[dictionary[w]] += 1\n",
    "        except:\n",
    "            i = i\n",
    "        i += 1\n",
    "        if(i+n > sLen):\n",
    "            break\n",
    "\n",
    "    return arr\n",
    "\n",
    "#Reads the DB files and puts the information of the file in a array of strings\n",
    "def readfile(filename):\n",
    "    temp = open(filename, 'r').read().split('\\n')\n",
    "    return temp\n",
    "\n",
    "\n",
    "#returns a random string of specified length\n",
    "#length: strign length\n",
    "def randomword(length):\n",
    "    return (''.join(choice('ACGT') for i in range(0, length)))\n",
    "\n",
    "#retuns an array of random strings\n",
    "#size: how many strings there will be in the array\n",
    "#lakeMinLen: min sequence length\n",
    "#lakeMaxLen: max sequence length\n",
    "def lakeString(size, lakeMinLen, lakeMaxLen):\n",
    "    lake_water = []\n",
    "    for i in range(0, size):\n",
    "        random.seed()\n",
    "        #generates a random sequence length\n",
    "        y = random.randint(lakeMinLen, lakeMaxLen)\n",
    "\n",
    "        _str = randomword(y)\n",
    "        lake_water.append(_str)\n",
    "    return lake_water\n",
    "\n",
    "\n",
    "def kdtree(data, lake_matrix, k_neighbors = 10, leaf_size = 20):\n",
    "    # training\n",
    "    # kdtree = KDTree(data, leaf_size=leaf_size, metric='minkowski')\n",
    "    kdtree = KDTree(data, leaf_size=leaf_size, metric='euclidean')\n",
    "\n",
    "    # testing\n",
    "    distances, indices = kdtree.query(lake_matrix, k=k_neighbors)\n",
    "    return np.array(indices), distances\n",
    "\n",
    "\n",
    "def clustering(data, lake_matrix, num_clusters = 12):\n",
    "    # training\n",
    "    estimator = KMeans(n_clusters=num_clusters)\n",
    "    estimator.fit(data)\n",
    "    training_labels = estimator.labels_\n",
    "\n",
    "    clusters = [[]]*num_clusters  # stores the index of the points in each cluster\n",
    "    for clust in range(0,num_clusters):\n",
    "        labels_idx = np.where(training_labels == clust)[0]\n",
    "        clusters[clust] = labels_idx\n",
    "\n",
    "#     # printing cluster data\n",
    "#     i = 0\n",
    "#     for clt in clusters:\n",
    "#         virs = [c for c in clt if c < len_viruses]\n",
    "#         perc = len(virs)/len(clt)\n",
    "#         print(\"% of virus in cluster \", i, \": \", perc)\n",
    "#         i += 1\n",
    "\n",
    "    # testing\n",
    "    indices = []\n",
    "    estimated_labels = estimator.predict(lake_matrix)\n",
    "    for lbl in estimated_labels:\n",
    "        indices.append(clusters[lbl])\n",
    "\n",
    "    return np.array(indices)\n",
    "\n",
    "def check_indices(indices, lake_filenames, filenames, lake_indices):\n",
    "    right = 0\n",
    "    cnt = 0\n",
    "\n",
    "    for i in lake_filenames:\n",
    "        for ii in indices[cnt]:\n",
    "            if i == filenames[ii]:\n",
    "                right += 1\n",
    "                break\n",
    "        cnt += 1\n",
    "    print (\"right answers: \", right, \" / \", len(lake_filenames))\n",
    "    return (right / len(lake_filenames))\n",
    "\n",
    "def listToString(list):\n",
    "    string = ''\n",
    "    for ll in list:\n",
    "        for l in ll:\n",
    "            string += str(l) + ' '\n",
    "        string += '\\n'\n",
    "    return string\n",
    "\n",
    "def filenameToMatrix(file):\n",
    "    l = []\n",
    "    fid = open(file,'r')\n",
    "    for f in fid:\n",
    "        l.append(f.replace('\\n',''))\n",
    "    return l\n",
    "\n",
    "def computeFeatureVector(known_bacterias):\n",
    "    import multiprocessing as mp\n",
    "    num_thread = multiprocessing.cpu_count()\n",
    "    matrix = [[]]*len(known_bacterias)\n",
    "    for i in range(num_thread):\n",
    "        featureVectorThread(matrix, i, D, w, n)\n",
    "\n",
    "    n = 4; D = CreateDictionary(n)\n",
    "    for w in known_bacterias:\n",
    "        arr = FeatureVector(D, str(w), n)\n",
    "        arr = np.divide(np.array(arr), len(w))\n",
    "        m+=1\n",
    "        if(m%50 == 0):\n",
    "            print(m,\"of\", len(known_bacterias))\n",
    "        bact_matrix.append(arr)\n",
    "\n",
    "\n",
    "# ******************************************************* main ****************************************************\n",
    "\n",
    "use_presaved = True\n",
    "\n",
    "if use_presaved:\n",
    "    virus_matrix = np.loadtxt('../presaved/virus_features.txt')\n",
    "    bact_matrix = np.loadtxt('../presaved/bact_features.txt')\n",
    "    virus_filenames = filenameToMatrix(\"../presaved/virus_filenames.txt\")\n",
    "    bact_filenames = filenameToMatrix(\"../presaved/bact_filenames.txt\")\n",
    "else:\n",
    "    print(\"reading data...\")\n",
    "    known_viruses, virus_filenames = ReadDataBaseFilenames(\"../database/virus/\", 0, \"../presaved/virus_filenames.txt\")\n",
    "    known_bacterias, bact_filenames = ReadDataBaseFilenames(\"../database/bact/\", 0, \"../presaved/bact_filenames.txt\")\n",
    "    virus_matrix = []\n",
    "    bact_matrix = []\n",
    "    n = 4; D = CreateDictionary(n)\n",
    "    print(\"generating feature vector virus...\")\n",
    "    for w in known_viruses:\n",
    "        arr = FeatureVector(D, str(w), n)\n",
    "        arr = np.divide(np.array(arr), len(w))\n",
    "        virus_matrix.append(arr)\n",
    "    m=0\n",
    "    print(\"generating feature vector bacteria...\")\n",
    "    for w in known_bacterias:\n",
    "        arr = FeatureVector(D, str(w), n)\n",
    "        arr = np.divide(np.array(arr), len(w))\n",
    "        m+=1\n",
    "        if(m%50 == 0):\n",
    "            print(m,\"of\", len(known_bacterias))\n",
    "        bact_matrix.append(arr)\n",
    "    # saving feature vectors in file\n",
    "    fid1 = open('../presaved/virus_features.txt', 'w')\n",
    "    fid2 = open('../presaved/bact_features.txt', 'w')\n",
    "    fid1.write(listToString(virus_matrix))\n",
    "    fid2.write(listToString(bact_matrix))\n",
    "    fid1.close(); fid2.close()\n",
    "\n",
    "\n",
    "use_pca = False\n",
    "pca_components = 50\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for it in range(1,35,5):\n",
    "# for it in [5,10]:\n",
    "\n",
    "    x_data.append(it)\n",
    "    print(\"===================== \", it)\n",
    "\n",
    "    # lake = ReadDataBase(\"../database/lake/\", 50)\n",
    "\n",
    "    print(\"generating lake...\")\n",
    "    lake_filenames, lake_indices = fl.Generate_lake(virus_filenames, bact_filenames, 400, 400)\n",
    "    lake = readfile(\"../database/lake.txt\")\n",
    "    num_virus_lake = int(lake[0])\n",
    "    num_bact_lake = int(lake[1])\n",
    "    lake = lake[2:]\n",
    "\n",
    "    print(\"finished reading data\")\n",
    "\n",
    "    lake_matrix = []\n",
    "\n",
    "    n = 4; D = CreateDictionary(n)\n",
    "    for w in lake:\n",
    "        arr = FeatureVector(D, str(w), n)\n",
    "        arr = np.divide(np.array(arr), len(w))\n",
    "        lake_matrix.append(arr)\n",
    "\n",
    "    len_lake = len(lake)\n",
    "    len_viruses = len(virus_matrix)\n",
    "\n",
    "    print(\"lake shape: \", np.matrix(lake_matrix).shape)\n",
    "    print(\"virus shape: \", np.matrix(virus_matrix).shape)\n",
    "    print(\"bacteria shape: \", np.matrix(bact_matrix).shape)\n",
    "\n",
    "    matrix = np.vstack((virus_matrix,bact_matrix))\n",
    "    virus_bact_filenames = np.hstack((virus_filenames,bact_filenames))\n",
    "\n",
    "    ###### PCA\n",
    "    if use_pca:\n",
    "        print(\"performing PCA...\")\n",
    "        X = np.array(matrix)\n",
    "        # PCA input: samples x features\n",
    "        pca = PCA(n_components=pca_components)\n",
    "        Xhat = pca.fit_transform(X)\n",
    "        print(\"Percentage of represented variance: \", sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "    ###### CLASSIFICATION\n",
    "    if use_pca:\n",
    "        data = np.array(Xhat)\n",
    "        lake_matrix = pca.transform(lake_matrix)\n",
    "    else:\n",
    "        data = np.array(matrix)\n",
    "\n",
    "\n",
    "    ### choose a classification method\n",
    "    ## the method will pick the best candidates to perform local alignment in each lake sample\n",
    "    perc = it/100\n",
    "    neighbors = round(perc*len(matrix))\n",
    "    print(\"running knn with \", neighbors, \" neighbors....\")\n",
    "    # neighbors = len(matrix)\n",
    "\n",
    "\n",
    "\n",
    "    indices, dist = kdtree(data, lake_matrix, k_neighbors = neighbors, leaf_size = 30)\n",
    "    # indices = clustering(data, lake_matrix, num_clusters = it)\n",
    "\n",
    "    print(\"indices: \", len(indices), len(indices[0]))\n",
    "\n",
    "    # print(\"lake atual: \", lake_filenames[6])\n",
    "    # print(\"dist:\")\n",
    "    # for i in range(0,100):\n",
    "    #     print(indices[6,i], \" | \", dist[6,i])\n",
    "\n",
    "    # for l in lake_filenames:\n",
    "    #     print(l)\n",
    "\n",
    "    acc = check_indices(indices, lake_filenames, virus_bact_filenames, lake_indices)\n",
    "    y_data.append(acc)\n",
    "\n",
    "print(\"x: \", x_data)\n",
    "print(\"y: \", y_data)\n",
    "\n",
    "plt.plot(x_data, y_data)\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Chance of the original sequence being a candidate')\n",
    "plt.xlabel('Number of neighbors (in percentage on the size of the training data)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ###### LOCAL ALIGNMENT\n",
    "# match = 1                                                  # scores 1 point for matching letters\n",
    "# mismatch = -1                                              # looses 1 point for mismatching letters\n",
    "# scoring = swalign.NucleotideScoringMatrix(match, mismatch)\n",
    "# sw = swalign.LocalAlignment(scoring)                       # you can also choose gap penalties, etc...\n",
    "# beta_x = np.arange(0, 1, 0.001)  # for beta distribution\n",
    "# _y = 1  # final curve\n",
    "# idx = 0\n",
    "# right = 0\n",
    "\n",
    "# print(\"performing alingment...\")\n",
    "# start = time.clock()\n",
    "# for lake_sample in lake:\n",
    "#     ## setup for local alignment\n",
    "#     viral_matches = 0\n",
    "#     bact_matches = 0\n",
    "#     sample_length = len(lake_sample)\n",
    "\n",
    "#     ## temporary databases\n",
    "#     virus_temp_db = []\n",
    "#     bact_temp_db = []\n",
    "#     print(\"performing with \", len(indices[idx]), \" samples\")\n",
    "#     for i in indices[idx]:\n",
    "#         if i < len_viruses:\n",
    "#             virus_temp_db.append(known_viruses[i])\n",
    "#         else:\n",
    "#             bact_temp_db.append(known_bacterias[i-len_viruses])\n",
    "\n",
    "#     if not(virus_temp_db):  # if virus is empty\n",
    "#         print(\"   all samples are bacteria: \", idx)\n",
    "#         bact_matches = 1\n",
    "#         viral_matches = 0\n",
    "#         virus_temp_db = []\n",
    "#         bact_temp_db = []\n",
    "#     if not(bact_temp_db):  # if bacteria is empty\n",
    "#         print(\"   all samples are viruses: \", idx)\n",
    "#         bact_matches = 0\n",
    "#         viral_matches = 1\n",
    "#         virus_temp_db = []\n",
    "#         bact_temp_db = []\n",
    "\n",
    "#     ## local alignment\n",
    "#     for v in virus_temp_db:\n",
    "#         alignment = sw.align(v, lake_sample)\n",
    "#         viral_matches += alignment.score\n",
    "#     for b in bact_temp_db:\n",
    "#         alignment = sw.align(b, lake_sample)\n",
    "#         bact_matches += alignment.score\n",
    "\n",
    "#     # checking answers\n",
    "#     if viral_matches > bact_matches and idx < len_viruses:\n",
    "#         right += 1\n",
    "#     if bact_matches > viral_matches and idx >= len_viruses:\n",
    "#         right += 1\n",
    "\n",
    "#     print(viral_matches, \" / \", bact_matches)\n",
    "\n",
    "#     idx += 1\n",
    "\n",
    "# end = time.clock()\n",
    "# print(\"\\n\\n>>>Elapsed Time:\", round(end - start, 3))\n",
    "# print(\"accuracy: \", right/(len_lake))\n",
    "\n",
    "# plt.savefig('myfig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
